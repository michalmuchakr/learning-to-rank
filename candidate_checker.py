#!/usr/bin/env python3
"""
Sprawdza podstawowy format zgÅ‚oszenia (nie jakoÅ›Ä‡!)
UÅ¼ycie: python candidate_checker.py ./moje_rozwiazanie/
"""

import pandas as pd
import json
import os
import sys

class CandidateChecker:
    def check_submission(self, submission_path):
        """SprawdÅº podstawowe wymagania zgÅ‚oszenia"""
        print("ğŸ” PODSTAWOWY SPRAWDZACZ ZGÅOSZENIA")
        print("=" * 50)
        print("âš ï¸  To sprawdza tylko format i completeness!")
        print("    Faktyczna ocena bÄ™dzie znacznie bardziej szczegÃ³Å‚owa.\n")
        
        if not os.path.exists(submission_path):
            print(f"âŒ Folder {submission_path} nie istnieje!")
            return False
        
        checks_passed = 0
        total_checks = 5

        print("ğŸ“ Sprawdzanie plikÃ³w...")
        required_files = ['results.json', 'predictions.csv', 'solution_summary.md']
        files_ok = True
        
        for file in required_files:
            if os.path.exists(os.path.join(submission_path, file)):
                print(f"âœ… {file} - znaleziono")
            else:
                print(f"âŒ {file} - BRAKUJE!")
                files_ok = False
        
        if files_ok:
            checks_passed += 1

        print("\nğŸ“Š Sprawdzanie results.json...")
        results_path = os.path.join(submission_path, 'results.json')
        if os.path.exists(results_path):
            try:
                with open(results_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)

                required_sections = ['candidate_info', 'data_analysis', 'model_performance', 'business_analysis']
                sections_ok = True
                
                for section in required_sections:
                    if section in results:
                        print(f"âœ… Sekcja '{section}' - obecna")
                    else:
                        print(f"âŒ Sekcja '{section}' - BRAKUJE!")
                        sections_ok = False
                
                if sections_ok:
                    checks_passed += 1

                if 'data_analysis' in results:
                    da = results['data_analysis']
                    required_metrics = [
                        'overall_ctr', 'position_bias_ratio', 'electronics_ctr', 
                        'quality_correlation', 'best_category'
                    ]
                    metrics_ok = True
                    
                    for metric in required_metrics:
                        if metric in da and da[metric] is not None:
                            print(f"âœ… Metryka '{metric}' - obecna")
                        else:
                            print(f"âŒ Metryka '{metric}' - BRAKUJE!")
                            metrics_ok = False
                    
                    if metrics_ok:
                        checks_passed += 1
                
            except Exception as e:
                print(f"âŒ BÅ‚Ä…d czytania results.json: {e}")

        print("\nğŸ“ˆ Sprawdzanie predictions.csv...")
        pred_path = os.path.join(submission_path, 'predictions.csv')
        if os.path.exists(pred_path):
            try:
                pred_df = pd.read_csv(pred_path)

                required_cols = ['session_id', 'product_id', 'actual_clicked', 'predicted_score']
                cols_ok = True
                
                for col in required_cols:
                    if col in pred_df.columns:
                        print(f"âœ… Kolumna '{col}' - obecna")
                    else:
                        print(f"âŒ Kolumna '{col}' - BRAKUJE!")
                        cols_ok = False
                
                if cols_ok:
                    checks_passed += 1

                    print(f"â„¹ï¸  Rozmiar zbioru testowego: {len(pred_df)} wierszy")
                    print(f"â„¹ï¸  Liczba sesji: {pred_df['session_id'].nunique()}")
                    
                    pred_min = pred_df['predicted_score'].min()
                    pred_max = pred_df['predicted_score'].max()
                    print(f"â„¹ï¸  Zakres predicted_score: {pred_min:.3f} - {pred_max:.3f}")
                    
                    if pred_max > pred_min + 0.01:
                        print("âœ… Model wydaje siÄ™ dyskryminowaÄ‡")
                    else:
                        print("âš ï¸  Model moÅ¼e nie dyskryminowaÄ‡ (wszystkie wyniki podobne)")
                
            except Exception as e:
                print(f"âŒ BÅ‚Ä…d czytania predictions.csv: {e}")

        print("\nğŸ¯ Sprawdzanie modelu...")
        try:
            if 'model_performance' in results:
                mp = results['model_performance']
                ndcg = float(mp.get('ndcg_at_5', 0))
                features_count = int(mp.get('features_count', 0))
                
                if 0.3 <= ndcg <= 1.0:
                    print(f"âœ… NDCG@5 w rozsÄ…dnym zakresie: {ndcg}")
                    checks_passed += 1
                else:
                    print(f"âš ï¸  NDCG@5 moÅ¼e byÄ‡ bÅ‚Ä™dne: {ndcg} (oczekiwane 0.3-1.0)")
                
                if features_count >= 5:
                    print(f"âœ… Liczba features OK: {features_count}")
                else:
                    print(f"âš ï¸  Za maÅ‚o features: {features_count} (wymagane â‰¥5)")
                    
        except Exception as e:
            print(f"âš ï¸  Nie moÅ¼na sprawdziÄ‡ metryki modelu: {e}")

        print(f"\nğŸ“Š PODSUMOWANIE:")
        print(f"Zaliczone sprawdzenia: {checks_passed}/{total_checks}")
        
        if checks_passed >= 4:
            print("âœ… GOTOWE DO WYSÅANIA!")
            print("   Format wyglÄ…da dobrze. MoÅ¼esz wysÅ‚aÄ‡ zgÅ‚oszenie.")
            return True
        elif checks_passed >= 2:
            print("âš ï¸  WYMAGA POPRAWEK")
            print("   CzÄ™Å›Ä‡ wymagaÅ„ nie speÅ‚niona. Popraw przed wysÅ‚aniem.")
            return False
        else:
            print("âŒ NIE GOTOWE")
            print("   WiÄ™kszoÅ›Ä‡ wymagaÅ„ nie speÅ‚niona. SprawdÅº instrukcje.")
            return False

def main():
    """GÅ‚Ã³wna funkcja checkera"""
    
    if len(sys.argv) != 2:
        print("ğŸ¯ ML ENGINEER CHALLENGE - CANDIDATE CHECKER")
        print("=" * 50)
        print("Sprawdza podstawowy format zgÅ‚oszenia (nie jakoÅ›Ä‡ analizy!)\\n")
        print("UÅ¼ycie: python candidate_checker.py <folder_zgloszczenia>")
        print("\\nPrzykÅ‚ad:")
        print("  python candidate_checker.py ./moje_rozwiazanie/")
        sys.exit(1)
    
    submission_path = sys.argv[1]
    
    checker = CandidateChecker()
    success = checker.check_submission(submission_path)
    
    print("\\n" + "="*50)
    if success:
        print("ğŸ‰ Format zgÅ‚oszenia wyglÄ…da dobrze!")
        print("ğŸ’¡ PamiÄ™taj: to tylko sprawdzenie formatu.")
        print("   JakoÅ›Ä‡ analizy bÄ™dzie oceniana osobno przez zespÃ³Å‚.")
    else:
        print("ğŸ”§ ZgÅ‚oszenie wymaga poprawek przed wysÅ‚aniem.")

if __name__ == "__main__":
    main()