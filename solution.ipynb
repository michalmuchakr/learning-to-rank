{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79c9fe4",
   "metadata": {},
   "source": [
    "# Learning-to-Rank Model for E-commerce Search\n",
    "\n",
    "This notebook implements a Learning-to-Rank solution for ranking products in an e-commerce search engine.\n",
    "\n",
    "## Tasks\n",
    "1. **Part 1: Data analysis** - Calculate key data analysis metrics\n",
    "2. **Part 2: Learning-to-rank model** - Build and evaluate the ranking model\n",
    "3. **Part 3: Business summary** - Business analysis and recommendations\n",
    "\n",
    "## Model features\n",
    "- **Required features**:\n",
    "  - `position_boost = 1/position` (clipped at position 3)\n",
    "  - `log_price = log(price_pln + 1)`\n",
    "  - `quality_price_ratio = quality_score / log_price`\n",
    "  - `category_match = (category == user_preferred_category) ? 1 : 0`\n",
    "  - Additional session-relative and interaction features\n",
    "\n",
    "- **Advanced features**:\n",
    "  - Session-relative features (price/quality rankings within session)\n",
    "  - Smoothed CTR priors computed on the training split only (Bayesian m-estimate)\n",
    "  - Position-bias control via buckets\n",
    "  - Category interactions\n",
    "  - Robust cross-validation evaluation\n",
    "\n",
    "## Outputs\n",
    "- `results.json` - Complete analysis results\n",
    "- `predictions.csv` - Model predictions on the test split\n",
    "- `solution_summary.md` - Solution summary\n",
    "\n",
    "**Expected input:** `search_sessions.csv` in the same directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68293b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (67651, 8)\n",
      "Sessions: 8000\n",
      "Products: 67651\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>position</th>\n",
       "      <th>clicked</th>\n",
       "      <th>price_pln</th>\n",
       "      <th>category</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>user_preferred_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40.05</td>\n",
       "      <td>Elektronika</td>\n",
       "      <td>0.363</td>\n",
       "      <td>Elektronika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>249.63</td>\n",
       "      <td>Elektronika</td>\n",
       "      <td>0.547</td>\n",
       "      <td>Elektronika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>679.19</td>\n",
       "      <td>Elektronika</td>\n",
       "      <td>0.696</td>\n",
       "      <td>Elektronika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>493.38</td>\n",
       "      <td>Elektronika</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Elektronika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2982.39</td>\n",
       "      <td>Odziez</td>\n",
       "      <td>0.384</td>\n",
       "      <td>Elektronika</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id product_id  position  clicked  price_pln     category  quality_score user_preferred_category\n",
       "0           0   prod_0_1         1        1      40.05  Elektronika          0.363             Elektronika\n",
       "1           0   prod_0_2         2        0     249.63  Elektronika          0.547             Elektronika\n",
       "2           0   prod_0_3         3        0     679.19  Elektronika          0.696             Elektronika\n",
       "3           0   prod_0_4         4        0     493.38  Elektronika          0.100             Elektronika\n",
       "4           0   prod_0_5         5        0    2982.39       Odziez          0.384             Elektronika"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install pandas numpy scikit-learn lightgbm matplotlib\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRanker, early_stopping, log_evaluation\n",
    "\n",
    "# Set logging level to ERROR\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logging.getLogger('lightgbm').setLevel(logging.ERROR)\n",
    "\n",
    "pd.set_option('display.max_columns', 400)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "DATA_PATH = \"search_sessions.csv\"\n",
    "assert os.path.exists(DATA_PATH), f\"Missing {DATA_PATH}. Put it next to this notebook.\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Basic coercions\n",
    "df[\"clicked\"] = df[\"clicked\"].astype(int)\n",
    "df[\"position\"] = df[\"position\"].astype(int)\n",
    "df[\"price_pln\"] = df[\"price_pln\"].astype(float)\n",
    "df[\"quality_score\"] = df[\"quality_score\"].astype(float)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Sessions: {df['session_id'].nunique()}\")\n",
    "print(f\"Products: {df['product_id'].nunique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5cfca",
   "metadata": {},
   "source": [
    "## Part 1: Data analysis\n",
    "\n",
    "Calculate the 5 key data analysis metrics required by the instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723e3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis Results:\n",
      "{\n",
      "  \"overall_ctr\": 0.0767,\n",
      "  \"position_bias_ratio\": 3.42,\n",
      "  \"electronics_ctr\": 0.1219,\n",
      "  \"quality_correlation\": 0.1198,\n",
      "  \"best_category\": \"Elektronika\"\n",
      "}\n",
      "\n",
      "CTR by position:\n",
      "position\n",
      "1     0.198625\n",
      "2     0.111000\n",
      "3     0.081500\n",
      "4     0.067375\n",
      "5     0.058000\n",
      "6     0.045318\n",
      "7     0.042628\n",
      "8     0.034455\n",
      "9     0.037443\n",
      "10    0.029592\n",
      "11    0.027211\n",
      "12    0.032191\n",
      "Name: clicked, dtype: float64\n",
      "\n",
      "CTR by category:\n",
      "category\n",
      "Elektronika    0.121921\n",
      "Ksiazki        0.036991\n",
      "Odziez         0.070694\n",
      "Name: clicked, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Data Analysis\n",
    "# Calculate the 5 required metrics\n",
    "\n",
    "# 1. overall_ctr: All clicks / all impressions\n",
    "overall_ctr = df[\"clicked\"].mean()\n",
    "\n",
    "# 2. position_bias_ratio: CTR position 1 / CTR position 5\n",
    "ctr_by_position = df.groupby(\"position\")[\"clicked\"].mean()\n",
    "position_bias_ratio = ctr_by_position[1] / ctr_by_position[5] if 5 in ctr_by_position.index and ctr_by_position[5] > 0 else 0.0\n",
    "\n",
    "# 3. electronics_ctr: CTR for category \"Elektronika\"\n",
    "electronics_ctr = df[df[\"category\"] == \"Elektronika\"][\"clicked\"].mean()\n",
    "\n",
    "# 4. quality_correlation: Correlation between quality_score and clicked\n",
    "quality_correlation = df[\"quality_score\"].corr(df[\"clicked\"])\n",
    "\n",
    "# 5. best_category: Category with highest CTR\n",
    "ctr_by_category = df.groupby(\"category\")[\"clicked\"].mean()\n",
    "best_category = ctr_by_category.idxmax()\n",
    "\n",
    "# Store results\n",
    "data_analysis = {\n",
    "    \"overall_ctr\": round(float(overall_ctr), 4),\n",
    "    \"position_bias_ratio\": round(float(position_bias_ratio), 2),\n",
    "    \"electronics_ctr\": round(float(electronics_ctr), 4),\n",
    "    \"quality_correlation\": round(float(quality_correlation), 4),\n",
    "    \"best_category\": str(best_category)\n",
    "}\n",
    "\n",
    "print(\"Data Analysis Results:\")\n",
    "print(json.dumps(data_analysis, indent=2))\n",
    "print(f\"\\nCTR by position:\\n{ctr_by_position}\")\n",
    "print(f\"\\nCTR by category:\\n{ctr_by_category}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645d431",
   "metadata": {},
   "source": [
    "## Part 2: Learning-to-rank model\n",
    "\n",
    "### Utilities and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead8e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-6\n",
    "\n",
    "# --- Metrics ---\n",
    "\n",
    "def ndcg_at_k(y_true, y_score, group_ids, k=5, ignore_no_positive=True):\n",
    "    \"\"\"Mean NDCG@k over groups.\n",
    "\n",
    "    - ignore_no_positive=True: skip sessions with no positive labels (IDCG=0)\n",
    "      This matches common LTR evaluation practice and typical library behavior.\n",
    "    - ignore_no_positive=False: include such sessions as 0.0.\n",
    "    \"\"\"\n",
    "    gdf = pd.DataFrame({\"g\": group_ids, \"y\": y_true, \"s\": y_score})\n",
    "    ndcgs = []\n",
    "    for _, part in gdf.groupby(\"g\", sort=False):\n",
    "        part = part.sort_values(\"s\", ascending=False)\n",
    "        rel = part[\"y\"].to_numpy()[:k]\n",
    "\n",
    "        discounts = np.log2(np.arange(2, rel.size + 2))\n",
    "        dcg = ((2**rel - 1) / discounts).sum()\n",
    "\n",
    "        ideal = np.sort(part[\"y\"].to_numpy())[::-1][:k]\n",
    "        idcg = ((2**ideal - 1) / discounts[: ideal.size]).sum()\n",
    "\n",
    "        if idcg == 0:\n",
    "            if ignore_no_positive:\n",
    "                continue\n",
    "            ndcgs.append(0.0)\n",
    "        else:\n",
    "            ndcgs.append(float(dcg / idcg))\n",
    "\n",
    "    return float(np.mean(ndcgs)) if ndcgs else 0.0\n",
    "\n",
    "\n",
    "# --- Splits & ordering ---\n",
    "\n",
    "def finalize_order(df):\n",
    "    \"\"\"Ensure stable ordering/contiguity by session_id before group size computation.\"\"\"\n",
    "    return df.sort_values([\"session_id\", \"position\", \"product_id\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def group_sizes(df, group_col=\"session_id\"):\n",
    "    # Requires df already sorted by group_col\n",
    "    return df.groupby(group_col, sort=False).size().to_numpy()\n",
    "\n",
    "\n",
    "def split_by_session(df, seed=42, test_size=0.2):\n",
    "    \"\"\"Split by session_id (no leakage). Returns (train_df, test_df).\"\"\"\n",
    "    sessions = df[\"session_id\"].unique()\n",
    "    train_sess, test_sess = train_test_split(sessions, test_size=test_size, random_state=seed, shuffle=True)\n",
    "    train_df = df[df[\"session_id\"].isin(train_sess)].copy()\n",
    "    test_df = df[df[\"session_id\"].isin(test_sess)].copy()\n",
    "    return finalize_order(train_df), finalize_order(test_df)\n",
    "\n",
    "\n",
    "def split_train_val_from_train(train_full_df, seed=42, val_fraction=0.1):\n",
    "    \"\"\"Create an internal validation split from the train portion only.\"\"\"\n",
    "    tr, va = split_by_session(train_full_df, seed=seed, test_size=val_fraction)\n",
    "    return tr, va\n",
    "\n",
    "\n",
    "# --- Feature utilities ---\n",
    "\n",
    "def m_estimate_ctr(train: pd.DataFrame, key_cols, label_col=\"clicked\", m=50.0):\n",
    "    prior = float(train[label_col].mean())\n",
    "    agg = train.groupby(key_cols, observed=False)[label_col].agg([\"sum\", \"count\"]).reset_index()\n",
    "    agg[\"ctr_prior\"] = (agg[\"sum\"] + m * prior) / (agg[\"count\"] + m)\n",
    "    return agg[key_cols + [\"ctr_prior\"]], float(prior)\n",
    "\n",
    "\n",
    "def clip_series(s, lo=-3.0, hi=3.0):\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "\n",
    "def zscore_in_group(x: pd.Series) -> pd.Series:\n",
    "    mu = x.mean()\n",
    "    std = x.std(ddof=0)\n",
    "    if (not np.isfinite(std)) or std <= 0:\n",
    "        return pd.Series(np.zeros(len(x), dtype=float), index=x.index)\n",
    "    out = (x - mu) / std\n",
    "    return out.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c443ed5",
   "metadata": {},
   "source": [
    "## 2) Feature Engineering v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30463f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>clicked</th>\n",
       "      <th>pos_boost_clipped3</th>\n",
       "      <th>posb_top3</th>\n",
       "      <th>posb_mid46</th>\n",
       "      <th>posb_bot710</th>\n",
       "      <th>log_price</th>\n",
       "      <th>quality_price_ratio</th>\n",
       "      <th>category_match</th>\n",
       "      <th>price_rank_in_session</th>\n",
       "      <th>quality_rank_in_session</th>\n",
       "      <th>price_pct_in_session</th>\n",
       "      <th>quality_pct_in_session</th>\n",
       "      <th>price_minus_session_median</th>\n",
       "      <th>quality_minus_session_mean</th>\n",
       "      <th>price_z_in_session_clipped</th>\n",
       "      <th>quality_z_in_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.714791</td>\n",
       "      <td>0.097717</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>-453.33</td>\n",
       "      <td>-0.146364</td>\n",
       "      <td>-0.697457</td>\n",
       "      <td>-0.636561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.523978</td>\n",
       "      <td>0.099023</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>-243.75</td>\n",
       "      <td>0.037636</td>\n",
       "      <td>-0.431507</td>\n",
       "      <td>0.163687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.522372</td>\n",
       "      <td>0.106710</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>185.81</td>\n",
       "      <td>0.186636</td>\n",
       "      <td>0.113590</td>\n",
       "      <td>0.811715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>6.203304</td>\n",
       "      <td>0.016120</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.409364</td>\n",
       "      <td>-0.122197</td>\n",
       "      <td>-1.780395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>prod_0_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>8.000816</td>\n",
       "      <td>0.047995</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>2489.01</td>\n",
       "      <td>-0.125364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.545229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id product_id  clicked  pos_boost_clipped3  posb_top3  posb_mid46  posb_bot710  log_price  quality_price_ratio  category_match  price_rank_in_session  quality_rank_in_session  \\\n",
       "0           0   prod_0_1        1            1.000000       True       False        False   3.714791             0.097717               1                    1.0                      9.0   \n",
       "1           0   prod_0_2        0            0.500000       True       False        False   5.523978             0.099023               1                    5.0                      6.0   \n",
       "2           0   prod_0_3        0            0.333333       True       False        False   6.522372             0.106710               1                   10.0                      3.0   \n",
       "3           0   prod_0_4        0            0.333333      False        True        False   6.203304             0.016120               1                    6.0                     11.0   \n",
       "4           0   prod_0_5        0            0.333333      False        True        False   8.000816             0.047995               0                   11.0                      8.0   \n",
       "\n",
       "   price_pct_in_session  quality_pct_in_session  price_minus_session_median  quality_minus_session_mean  price_z_in_session_clipped  quality_z_in_session  \n",
       "0              0.090909                0.818182                     -453.33                   -0.146364                   -0.697457             -0.636561  \n",
       "1              0.454545                0.545455                     -243.75                    0.037636                   -0.431507              0.163687  \n",
       "2              0.909091                0.272727                      185.81                    0.186636                    0.113590              0.811715  \n",
       "3              0.545455                1.000000                        0.00                   -0.409364                   -0.122197             -1.780395  \n",
       "4              1.000000                0.727273                     2489.01                   -0.125364                    3.000000             -0.545229  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_position_features(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = d.copy()\n",
    "    out[\"position_bucket\"] = pd.cut(\n",
    "        out[\"position\"],\n",
    "        bins=[0,3,6,10],\n",
    "        labels=[\"top3\",\"mid46\",\"bot710\"],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    pb = pd.get_dummies(out[\"position_bucket\"], prefix=\"posb\")\n",
    "    out = pd.concat([out, pb], axis=1)\n",
    "    out[\"pos_boost_clipped3\"] = 1.0 / out[\"position\"].clip(upper=3).astype(float)\n",
    "    return out\n",
    "\n",
    "def session_relative_features(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = d.copy()\n",
    "    out[\"session_size\"] = out.groupby(\"session_id\")[\"product_id\"].transform(\"size\")\n",
    "\n",
    "    # price: lower is \"better\" (rank 1 = cheapest)\n",
    "    out[\"price_rank_in_session\"] = out.groupby(\"session_id\")[\"price_pln\"].rank(method=\"average\", ascending=True)\n",
    "    out[\"price_pct_in_session\"] = out[\"price_rank_in_session\"] / out[\"session_size\"].replace(0, 1)\n",
    "    out[\"price_min_in_session\"] = out.groupby(\"session_id\")[\"price_pln\"].transform(\"min\")\n",
    "    out[\"price_max_in_session\"] = out.groupby(\"session_id\")[\"price_pln\"].transform(\"max\")\n",
    "    out[\"is_cheapest_in_session\"] = (out[\"price_pln\"] == out[\"price_min_in_session\"]).astype(int)\n",
    "    out[\"is_most_expensive_in_session\"] = (out[\"price_pln\"] == out[\"price_max_in_session\"]).astype(int)\n",
    "\n",
    "    out[\"price_median_in_session\"] = out.groupby(\"session_id\")[\"price_pln\"].transform(\"median\")\n",
    "    out[\"price_minus_session_median\"] = out[\"price_pln\"] - out[\"price_median_in_session\"]\n",
    "\n",
    "    out[\"price_z_in_session\"] = out.groupby(\"session_id\")[\"price_pln\"].transform(zscore_in_group).astype(float)\n",
    "    out[\"price_z_in_session_clipped\"] = clip_series(out[\"price_z_in_session\"], -3, 3)\n",
    "\n",
    "    # quality: higher is \"better\" (rank 1 = highest quality)\n",
    "    out[\"quality_rank_in_session\"] = out.groupby(\"session_id\")[\"quality_score\"].rank(method=\"average\", ascending=False)\n",
    "    out[\"quality_pct_in_session\"] = out[\"quality_rank_in_session\"] / out[\"session_size\"].replace(0, 1)\n",
    "    out[\"quality_min_in_session\"] = out.groupby(\"session_id\")[\"quality_score\"].transform(\"min\")\n",
    "    out[\"quality_max_in_session\"] = out.groupby(\"session_id\")[\"quality_score\"].transform(\"max\")\n",
    "    out[\"is_best_quality_in_session\"] = (out[\"quality_score\"] == out[\"quality_max_in_session\"]).astype(int)\n",
    "    out[\"is_worst_quality_in_session\"] = (out[\"quality_score\"] == out[\"quality_min_in_session\"]).astype(int)\n",
    "\n",
    "    out[\"quality_mean_in_session\"] = out.groupby(\"session_id\")[\"quality_score\"].transform(\"mean\")\n",
    "    out[\"quality_minus_session_mean\"] = out[\"quality_score\"] - out[\"quality_mean_in_session\"]\n",
    "\n",
    "    out[\"quality_z_in_session\"] = out.groupby(\"session_id\")[\"quality_score\"].transform(zscore_in_group).astype(float)\n",
    "\n",
    "    out[\"category_freq_in_session\"] = out.groupby([\"session_id\", \"category\"])[\"product_id\"].transform(\"size\")\n",
    "\n",
    "    counts = out.groupby([\"session_id\", \"category\"], observed=False).size().reset_index(name=\"cnt\")\n",
    "    counts[\"max_cnt\"] = counts.groupby(\"session_id\")[\"cnt\"].transform(\"max\")\n",
    "    counts[\"is_majority_category\"] = (counts[\"cnt\"] == counts[\"max_cnt\"]).astype(int)\n",
    "    out = out.merge(counts[[\"session_id\", \"category\", \"is_majority_category\"]], on=[\"session_id\", \"category\"], how=\"left\")\n",
    "    out[\"is_majority_category\"] = out[\"is_majority_category\"].fillna(0).astype(int)\n",
    "\n",
    "    return out\n",
    "\n",
    "def add_core_features(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = d.copy()\n",
    "    out[\"log_price\"] = np.log(out[\"price_pln\"] + 1.0)\n",
    "    out[\"category_match\"] = (out[\"category\"] == out[\"user_preferred_category\"]).astype(int)\n",
    "    out[\"quality_price_ratio\"] = out[\"quality_score\"] / out[\"log_price\"].replace(0.0, EPS)\n",
    "    return out\n",
    "\n",
    "def one_hot_categories(train: pd.DataFrame, test: pd.DataFrame):\n",
    "    # Drop one level to avoid structural multicollinearity (one-hot sums to 1)\n",
    "    # This also removes several exact linear dependencies with interaction features.\n",
    "    cat_train = pd.get_dummies(train[\"category\"], prefix=\"cat\", drop_first=True)\n",
    "    pref_train = pd.get_dummies(train[\"user_preferred_category\"], prefix=\"pref\", drop_first=True)\n",
    "\n",
    "    cat_test = pd.get_dummies(test[\"category\"], prefix=\"cat\").reindex(columns=cat_train.columns, fill_value=0)\n",
    "    pref_test = pd.get_dummies(test[\"user_preferred_category\"], prefix=\"pref\").reindex(columns=pref_train.columns, fill_value=0)\n",
    "\n",
    "    train2 = pd.concat([train, cat_train, pref_train], axis=1)\n",
    "    test2  = pd.concat([test, cat_test, pref_test], axis=1)\n",
    "    return train2, test2, list(cat_train.columns), list(pref_train.columns)\n",
    "\n",
    "def add_ctr_priors(train: pd.DataFrame, test: pd.DataFrame, m=50.0):\n",
    "    global_ctr = train[\"clicked\"].mean()\n",
    "\n",
    "    map_cat, _ = m_estimate_ctr(train, [\"category\"], m=m)\n",
    "    train = train.merge(map_cat.rename(columns={\"ctr_prior\":\"ctr_prior_category\"}), on=[\"category\"], how=\"left\")\n",
    "    test  = test.merge(map_cat.rename(columns={\"ctr_prior\":\"ctr_prior_category\"}), on=[\"category\"], how=\"left\")\n",
    "\n",
    "    map_cxp, _ = m_estimate_ctr(train, [\"category\",\"user_preferred_category\"], m=m)\n",
    "    train = train.merge(map_cxp.rename(columns={\"ctr_prior\":\"ctr_prior_cat_x_pref\"}), on=[\"category\",\"user_preferred_category\"], how=\"left\")\n",
    "    test  = test.merge(map_cxp.rename(columns={\"ctr_prior\":\"ctr_prior_cat_x_pref\"}), on=[\"category\",\"user_preferred_category\"], how=\"left\")\n",
    "\n",
    "    map_pref, _ = m_estimate_ctr(train, [\"user_preferred_category\"], m=m)\n",
    "    train = train.merge(map_pref.rename(columns={\"ctr_prior\":\"ctr_prior_pref\"}), on=[\"user_preferred_category\"], how=\"left\")\n",
    "    test  = test.merge(map_pref.rename(columns={\"ctr_prior\":\"ctr_prior_pref\"}), on=[\"user_preferred_category\"], how=\"left\")\n",
    "\n",
    "    map_cpb, _ = m_estimate_ctr(train, [\"category\",\"position_bucket\"], m=m)\n",
    "    train = train.merge(map_cpb.rename(columns={\"ctr_prior\":\"ctr_prior_cat_x_posb\"}), on=[\"category\",\"position_bucket\"], how=\"left\")\n",
    "    test  = test.merge(map_cpb.rename(columns={\"ctr_prior\":\"ctr_prior_cat_x_posb\"}), on=[\"category\",\"position_bucket\"], how=\"left\")\n",
    "\n",
    "    for d in (train, test):\n",
    "        d[\"ctr_prior_category\"] = d[\"ctr_prior_category\"].fillna(global_ctr)\n",
    "        d[\"ctr_prior_cat_x_pref\"] = d[\"ctr_prior_cat_x_pref\"].fillna(global_ctr)\n",
    "        d[\"ctr_prior_pref\"] = d[\"ctr_prior_pref\"].fillna(global_ctr)\n",
    "        d[\"ctr_prior_cat_x_posb\"] = d[\"ctr_prior_cat_x_posb\"].fillna(global_ctr)\n",
    "\n",
    "    return train, test, float(global_ctr)\n",
    "\n",
    "def add_interactions(d: pd.DataFrame, cat_cols) -> pd.DataFrame:\n",
    "    out = d.copy()\n",
    "\n",
    "    out[\"match_x_posb_top3\"] = out[\"category_match\"] * out.get(\"posb_top3\", 0)\n",
    "    out[\"match_x_posb_mid46\"] = out[\"category_match\"] * out.get(\"posb_mid46\", 0)\n",
    "    out[\"match_x_posb_bot710\"] = out[\"category_match\"] * out.get(\"posb_bot710\", 0)\n",
    "\n",
    "    out[\"quality_rank_x_match\"] = out[\"quality_rank_in_session\"] * out[\"category_match\"]\n",
    "    out[\"price_rank_x_match\"] = out[\"price_rank_in_session\"] * out[\"category_match\"]\n",
    "    out[\"quality_pct_x_match\"] = out[\"quality_pct_in_session\"] * out[\"category_match\"]\n",
    "    out[\"price_pct_x_match\"] = out[\"price_pct_in_session\"] * out[\"category_match\"]\n",
    "\n",
    "    out[\"quality_x_log_price\"] = out[\"quality_score\"] * out[\"log_price\"]\n",
    "\n",
    "    for c in cat_cols:\n",
    "        out[f\"{c}_x_quality\"] = out[c] * out[\"quality_score\"]\n",
    "        out[f\"{c}_x_log_price\"] = out[c] * out[\"log_price\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "def build_features_v4(train_raw: pd.DataFrame, test_raw: pd.DataFrame, m=50.0, use_ipw=False):\n",
    "    train = add_core_features(train_raw)\n",
    "    test  = add_core_features(test_raw)\n",
    "\n",
    "    train = add_position_features(train)\n",
    "    test  = add_position_features(test)\n",
    "\n",
    "    train = session_relative_features(train)\n",
    "    test  = session_relative_features(test)\n",
    "\n",
    "    train, test, cat_cols, pref_cols = one_hot_categories(train, test)\n",
    "\n",
    "    train, test, global_ctr = add_ctr_priors(train, test, m=m)\n",
    "\n",
    "    train = add_interactions(train, cat_cols)\n",
    "    test  = add_interactions(test, cat_cols)\n",
    "\n",
    "    if use_ipw:\n",
    "        prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
    "        prop = {str(k): v for k, v in prop.items()}  # Convert dict keys to strings\n",
    "        p_train = train[\"position_bucket\"].astype(str).map(prop).astype(float).fillna(train[\"clicked\"].mean()).clip(0.01, 0.99)\n",
    "        p_test  = test[\"position_bucket\"].astype(str).map(prop).astype(float).fillna(train[\"clicked\"].mean()).clip(0.01, 0.99)\n",
    "        train[\"ipw\"] = 1.0 / p_train\n",
    "        test[\"ipw\"] = 1.0 / p_test\n",
    "    else:\n",
    "        train[\"ipw\"] = 1.0\n",
    "        test[\"ipw\"] = 1.0\n",
    "\n",
    "    base = [\n",
    "        \"pos_boost_clipped3\",\"posb_top3\",\"posb_mid46\",\"posb_bot710\",\n",
    "        \"log_price\",\"quality_price_ratio\",\"category_match\",\n",
    "        \"price_rank_in_session\",\"quality_rank_in_session\",\n",
    "        \"price_pct_in_session\",\"quality_pct_in_session\",\n",
    "        \"price_minus_session_median\",\"quality_minus_session_mean\",\n",
    "        \"price_z_in_session_clipped\",\"quality_z_in_session\",\n",
    "        \"is_cheapest_in_session\",\"is_most_expensive_in_session\",\n",
    "        \"is_best_quality_in_session\",\"is_worst_quality_in_session\",\n",
    "        \"session_size\",\"category_freq_in_session\",\"is_majority_category\",\n",
    "        \"ctr_prior_category\",\"ctr_prior_cat_x_pref\",\"ctr_prior_pref\",\"ctr_prior_cat_x_posb\",\n",
    "        \"match_x_posb_top3\",\"match_x_posb_mid46\",\"match_x_posb_bot710\",\n",
    "        \"quality_rank_x_match\",\"price_rank_x_match\",\"quality_pct_x_match\",\"price_pct_x_match\",\n",
    "    ]\n",
    "\n",
    "    extra = [c for c in train.columns if c.startswith(\"cat_\") or c.startswith(\"pref_\") or c.endswith(\"_x_quality\") or c.endswith(\"_x_log_price\")]\n",
    "    feature_cols = base + extra\n",
    "\n",
    "    for c in feature_cols:\n",
    "        if c not in train.columns: train[c] = 0\n",
    "        if c not in test.columns: test[c] = 0\n",
    "\n",
    "    # Critical: enforce contiguity by session_id before group size computation\n",
    "    train = finalize_order(train)\n",
    "    test  = finalize_order(test)\n",
    "\n",
    "    return train, test, feature_cols, global_ctr\n",
    "\n",
    "# 80/20 split by session_id (as required). Validation is taken from the 80% train portion only.\n",
    "train_full_raw, test_raw = split_by_session(df, seed=42)\n",
    "train_raw, val_raw = split_train_val_from_train(train_full_raw, seed=42, val_fraction=0.1)\n",
    "\n",
    "# Fit features on train only; apply to val/test (priors computed on train only)\n",
    "train_fe, val_fe, feature_cols, global_ctr = build_features_v4(train_raw, val_raw, m=50.0, use_ipw=False)\n",
    "_, test_fe, _, _ = build_features_v4(train_raw, test_raw, m=50.0, use_ipw=False)\n",
    "\n",
    "print(\"Feature count:\", len(feature_cols))\n",
    "train_fe[[\"session_id\",\"product_id\",\"clicked\"] + feature_cols[:15]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bee382",
   "metadata": {},
   "source": [
    "## 3) Train LightGBM Ranker (stability-focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883bda45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3229\n",
      "[LightGBM] [Info] Number of data points in the train set: 48698, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's ndcg@5: 0.833266\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's ndcg@5: 0.84183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2968\n",
      "[LightGBM] [Info] Number of data points in the train set: 48698, number of used features: 38\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.83286\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's ndcg@5: 0.840961\n",
      "Auto-pruning: dropped 4 features (abs spearman corr >= 0.97); kept 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropped</th>\n",
       "      <th>kept</th>\n",
       "      <th>abs_corr</th>\n",
       "      <th>method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quality_z_in_session</td>\n",
       "      <td>quality_minus_session_mean</td>\n",
       "      <td>0.986963</td>\n",
       "      <td>spearman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is_majority_category</td>\n",
       "      <td>category_match</td>\n",
       "      <td>0.980409</td>\n",
       "      <td>spearman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat_Odziez</td>\n",
       "      <td>cat_Odziez_x_quality</td>\n",
       "      <td>0.973709</td>\n",
       "      <td>spearman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat_Ksiazki</td>\n",
       "      <td>cat_Ksiazki_x_quality</td>\n",
       "      <td>0.973580</td>\n",
       "      <td>spearman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dropped                        kept  abs_corr    method\n",
       "0  quality_z_in_session  quality_minus_session_mean  0.986963  spearman\n",
       "1  is_majority_category              category_match  0.980409  spearman\n",
       "2            cat_Odziez        cat_Odziez_x_quality  0.973709  spearman\n",
       "3           cat_Ksiazki       cat_Ksiazki_x_quality  0.973580  spearman"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 train (exclude no-click sessions): 0.7234991122364082\n",
      "NDCG@5 val   (exclude no-click sessions): 0.6618430258384693\n",
      "NDCG@5 test  (exclude no-click sessions): 0.6221961635508589\n",
      "NDCG@5 test (include all sessions, no-click => 0): 0.2935988146755616\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quality_minus_session_mean</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_x_log_price</th>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_price_ratio</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_z_in_session_clipped</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_price</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_pct_in_session</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_freq_in_session</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctr_prior_cat_x_pref</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_minus_session_median</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctr_prior_cat_x_posb</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_boost_clipped3</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_Ksiazki_x_quality</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_pct_in_session</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_pct_x_match</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_rank_in_session</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_pct_x_match</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_rank_x_match</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_Odziez_x_log_price</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_Odziez_x_quality</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_Ksiazki_x_log_price</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_rank_x_match</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posb_bot710</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_rank_in_session</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_x_posb_top3</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctr_prior_category</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            importance\n",
       "quality_minus_session_mean         223\n",
       "quality_x_log_price                134\n",
       "quality_price_ratio                122\n",
       "price_z_in_session_clipped          98\n",
       "log_price                           92\n",
       "quality_pct_in_session              86\n",
       "category_freq_in_session            66\n",
       "ctr_prior_cat_x_pref                65\n",
       "price_minus_session_median          64\n",
       "ctr_prior_cat_x_posb                51\n",
       "pos_boost_clipped3                  51\n",
       "cat_Ksiazki_x_quality               46\n",
       "price_pct_in_session                44\n",
       "price_pct_x_match                   42\n",
       "price_rank_in_session               35\n",
       "quality_pct_x_match                 32\n",
       "quality_rank_x_match                28\n",
       "cat_Odziez_x_log_price              28\n",
       "cat_Odziez_x_quality                27\n",
       "cat_Ksiazki_x_log_price             26\n",
       "price_rank_x_match                  22\n",
       "posb_bot710                         22\n",
       "quality_rank_in_session             21\n",
       "match_x_posb_top3                   18\n",
       "ctr_prior_category                  15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Automatic correlated-feature pruning (optional) ---\n",
    "# Train a baseline model to get importances, drop one feature from each highly-correlated pair,\n",
    "# then re-train.\n",
    "AUTO_PRUNE_CORRELATED = True\n",
    "PRUNE_CORR_METHOD = \"spearman\"   # \"spearman\" (monotonic), or \"pearson\" (linear)\n",
    "PRUNE_CORR_THRESHOLD = 0.97\n",
    "PRUNE_MIN_FEATURES = 20\n",
    "\n",
    "\n",
    "def train_ranker(train_fe, val_fe, feature_cols, seed=42, use_weights=False):\n",
    "    \"\"\"Train on train_fe, early-stop on val_fe. Do not use the final test set for model selection.\"\"\"\n",
    "    X_train = train_fe[feature_cols]\n",
    "    y_train = train_fe[\"clicked\"].astype(int)\n",
    "    X_val = val_fe[feature_cols]\n",
    "    y_val = val_fe[\"clicked\"].astype(int)\n",
    "\n",
    "    train_group = group_sizes(train_fe, \"session_id\")\n",
    "    val_group = group_sizes(val_fe, \"session_id\")\n",
    "\n",
    "    model = LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        ndcg_eval_at=[5],\n",
    "        n_estimators=8000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=63,\n",
    "        min_data_in_leaf=200,\n",
    "        min_child_samples=None,  # Explicitly disable to avoid warning when min_data_in_leaf is set\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        reg_lambda=2.0,\n",
    "        reg_alpha=0.5,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    fit_kwargs = dict(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        group=train_group,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_group=[val_group],\n",
    "        eval_at=[5],\n",
    "        callbacks=[early_stopping(stopping_rounds=300), log_evaluation(period=300)],\n",
    "    )\n",
    "\n",
    "    if use_weights:\n",
    "        fit_kwargs[\"sample_weight\"] = train_fe[\"ipw\"].astype(float)\n",
    "        fit_kwargs[\"eval_sample_weight\"] = [val_fe[\"ipw\"].astype(float)]\n",
    "\n",
    "    model.fit(**fit_kwargs)\n",
    "\n",
    "    imp = pd.Series(model.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "    return model, imp\n",
    "\n",
    "\n",
    "def _feature_priority(name: str, importance: float) -> tuple[int, float]:\n",
    "    \"\"\"Higher is better. Prefer base-like features over one-hots/interactions if importance is similar.\"\"\"\n",
    "    base_like = int(not (name.startswith(\"cat_\") or name.startswith(\"pref_\") or (\"_x_\" in name)))\n",
    "    return (base_like, float(importance))\n",
    "\n",
    "\n",
    "def prune_correlated_features(\n",
    "    train_fe: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    importance: pd.Series,\n",
    "    method: str = \"spearman\",\n",
    "    threshold: float = 0.97,\n",
    "    min_features: int = 20,\n",
    "):\n",
    "    X = (\n",
    "        train_fe[feature_cols]\n",
    "        .apply(pd.to_numeric, errors=\"coerce\")\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    corr = X.corr(method=method).abs()\n",
    "    cols = list(corr.columns)\n",
    "\n",
    "    pairs = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            v = float(corr.iat[i, j])\n",
    "            if np.isfinite(v) and v >= threshold:\n",
    "                pairs.append((v, cols[i], cols[j]))\n",
    "\n",
    "    pairs.sort(reverse=True)  # highest first\n",
    "\n",
    "    keep = set(feature_cols)\n",
    "    dropped_rows = []\n",
    "    imp = importance.to_dict()\n",
    "\n",
    "    for v, a, b in pairs:\n",
    "        if len(keep) <= min_features:\n",
    "            break\n",
    "        if a not in keep or b not in keep:\n",
    "            continue\n",
    "\n",
    "        pa = _feature_priority(a, imp.get(a, 0.0))\n",
    "        pb = _feature_priority(b, imp.get(b, 0.0))\n",
    "\n",
    "        # Drop lower-priority; on tie drop b (stable)\n",
    "        drop = a if pa < pb else b\n",
    "        kept = b if drop == a else a\n",
    "\n",
    "        keep.remove(drop)\n",
    "        dropped_rows.append({\"dropped\": drop, \"kept\": kept, \"abs_corr\": float(v), \"method\": method})\n",
    "\n",
    "    pruned_cols = [c for c in feature_cols if c in keep]\n",
    "    dropped_df = (\n",
    "        pd.DataFrame(dropped_rows).sort_values(\"abs_corr\", ascending=False)\n",
    "        if dropped_rows\n",
    "        else pd.DataFrame(columns=[\"dropped\", \"kept\", \"abs_corr\", \"method\"])\n",
    "    )\n",
    "\n",
    "    return pruned_cols, dropped_df\n",
    "\n",
    "\n",
    "def train_ranker_with_pruning(\n",
    "    train_fe,\n",
    "    val_fe,\n",
    "    feature_cols,\n",
    "    seed=42,\n",
    "    use_weights=False,\n",
    "    prune_corr=AUTO_PRUNE_CORRELATED,\n",
    "    prune_method=PRUNE_CORR_METHOD,\n",
    "    prune_threshold=PRUNE_CORR_THRESHOLD,\n",
    "    prune_min_features=PRUNE_MIN_FEATURES,\n",
    "):\n",
    "    # 1) baseline fit for importances\n",
    "    model_full, imp_full = train_ranker(train_fe, val_fe, feature_cols, seed=seed, use_weights=use_weights)\n",
    "\n",
    "    if not prune_corr:\n",
    "        return model_full, imp_full, feature_cols, pd.DataFrame(columns=[\"dropped\", \"kept\", \"abs_corr\", \"method\"])\n",
    "\n",
    "    # 2) prune on TRAIN split only\n",
    "    pruned_cols, dropped_df = prune_correlated_features(\n",
    "        train_fe,\n",
    "        feature_cols,\n",
    "        imp_full,\n",
    "        method=prune_method,\n",
    "        threshold=prune_threshold,\n",
    "        min_features=prune_min_features,\n",
    "    )\n",
    "\n",
    "    if len(pruned_cols) == len(feature_cols):\n",
    "        return model_full, imp_full, feature_cols, dropped_df\n",
    "\n",
    "    # 3) retrain on pruned feature set\n",
    "    model, imp = train_ranker(train_fe, val_fe, pruned_cols, seed=seed, use_weights=use_weights)\n",
    "    return model, imp, pruned_cols, dropped_df\n",
    "\n",
    "\n",
    "def eval_ranker(model, fe, feature_cols, k=5):\n",
    "    X = fe[feature_cols]\n",
    "    y = fe[\"clicked\"].astype(int).to_numpy()\n",
    "    sess = fe[\"session_id\"].to_numpy()\n",
    "\n",
    "    pred = model.predict(X, num_iteration=model.best_iteration_)\n",
    "\n",
    "    ndcg_excl_no_click = ndcg_at_k(y, pred, sess, k=k, ignore_no_positive=True)\n",
    "    ndcg_all_sessions = ndcg_at_k(y, pred, sess, k=k, ignore_no_positive=False)\n",
    "\n",
    "    return pred, float(ndcg_excl_no_click), float(ndcg_all_sessions)\n",
    "\n",
    "\n",
    "model, imp, feature_cols, dropped_corr = train_ranker_with_pruning(\n",
    "    train_fe, val_fe, feature_cols, seed=42, use_weights=False\n",
    ")\n",
    "\n",
    "if AUTO_PRUNE_CORRELATED:\n",
    "    print(\n",
    "        f\"Auto-pruning: dropped {len(dropped_corr)} features \"\n",
    "        f\"(abs {PRUNE_CORR_METHOD} corr >= {PRUNE_CORR_THRESHOLD}); kept {len(feature_cols)}\"\n",
    "    )\n",
    "    if len(dropped_corr):\n",
    "        display(dropped_corr.head(25))\n",
    "\n",
    "pred_train, ndcg_train, ndcg_train_all = eval_ranker(model, train_fe, feature_cols, k=5)\n",
    "pred_val, ndcg_val, ndcg_val_all = eval_ranker(model, val_fe, feature_cols, k=5)\n",
    "pred, ndcg_test, ndcg_test_all = eval_ranker(model, test_fe, feature_cols, k=5)\n",
    "\n",
    "print(\"NDCG@5 train (exclude no-click sessions):\", ndcg_train)\n",
    "print(\"NDCG@5 val   (exclude no-click sessions):\", ndcg_val)\n",
    "print(\"NDCG@5 test  (exclude no-click sessions):\", ndcg_test)\n",
    "\n",
    "print(\"NDCG@5 test (include all sessions, no-click => 0):\", ndcg_test_all)\n",
    "\n",
    "display(imp.head(25).to_frame(\"importance\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a4afc",
   "metadata": {},
   "source": [
    "## 4) Robust evaluation: multi-split CV (mean  std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8200e179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3229\n",
      "[LightGBM] [Info] Number of data points in the train set: 48690, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.810665\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.821421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2713\n",
      "[LightGBM] [Info] Number of data points in the train set: 48690, number of used features: 37\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.805974\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@5: 0.818302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3234\n",
      "[LightGBM] [Info] Number of data points in the train set: 48703, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.817622\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's ndcg@5: 0.828105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2973\n",
      "[LightGBM] [Info] Number of data points in the train set: 48703, number of used features: 38\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.814411\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's ndcg@5: 0.828887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3230\n",
      "[LightGBM] [Info] Number of data points in the train set: 48658, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.824937\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's ndcg@5: 0.830073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 48658, number of used features: 38\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.826509\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's ndcg@5: 0.832446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3234\n",
      "[LightGBM] [Info] Number of data points in the train set: 48623, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.823675\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@5: 0.837284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2973\n",
      "[LightGBM] [Info] Number of data points in the train set: 48623, number of used features: 38\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.828681\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's ndcg@5: 0.836097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3233\n",
      "[LightGBM] [Info] Number of data points in the train set: 48694, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.828638\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@5: 0.835197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2717\n",
      "[LightGBM] [Info] Number of data points in the train set: 48694, number of used features: 37\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.832037\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's ndcg@5: 0.836751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>ndcg@5_train</th>\n",
       "      <th>ndcg@5_val</th>\n",
       "      <th>ndcg@5_test</th>\n",
       "      <th>ndcg@5_test_all_sessions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0.676627</td>\n",
       "      <td>0.628476</td>\n",
       "      <td>0.650277</td>\n",
       "      <td>0.306036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.679630</td>\n",
       "      <td>0.670143</td>\n",
       "      <td>0.639958</td>\n",
       "      <td>0.296781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>0.681991</td>\n",
       "      <td>0.638941</td>\n",
       "      <td>0.653875</td>\n",
       "      <td>0.322851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>0.685472</td>\n",
       "      <td>0.657196</td>\n",
       "      <td>0.647425</td>\n",
       "      <td>0.309550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>0.729441</td>\n",
       "      <td>0.654043</td>\n",
       "      <td>0.636519</td>\n",
       "      <td>0.299960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  ndcg@5_train  ndcg@5_val  ndcg@5_test  ndcg@5_test_all_sessions\n",
       "0    11      0.676627    0.628476     0.650277                  0.306036\n",
       "1    22      0.679630    0.670143     0.639958                  0.296781\n",
       "2    33      0.681991    0.638941     0.653875                  0.322851\n",
       "3    44      0.685472    0.657196     0.647425                  0.309550\n",
       "4    55      0.729441    0.654043     0.636519                  0.299960"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean  std:\n",
      "NDCG@5 test: 0.6456107897075943  0.007210532727294305\n",
      "NDCG@5 test (include all sessions, no-click => 0): 0.3070355086114548  0.010157155846427105\n"
     ]
    }
   ],
   "source": [
    "# Robust evaluation: multi-split CV (mean  std)\n",
    "# Train: session-based split, early stopping on validation split carved out of train.\n",
    "\n",
    "\n",
    "def run_cv(df, seeds=(11, 22, 33, 44, 55), m=50.0, use_ipw=False, use_weights=False, val_fraction=0.1):\n",
    "    rows = []\n",
    "    for seed in seeds:\n",
    "        train_full_raw, test_raw = split_by_session(df, seed=seed)\n",
    "        train_raw, val_raw = split_train_val_from_train(train_full_raw, seed=seed, val_fraction=val_fraction)\n",
    "\n",
    "        train_fe, val_fe, feats, _ = build_features_v4(train_raw, val_raw, m=m, use_ipw=use_ipw)\n",
    "        _, test_fe, _, _ = build_features_v4(train_raw, test_raw, m=m, use_ipw=use_ipw)\n",
    "\n",
    "        model, _, feats_used, _ = train_ranker_with_pruning(\n",
    "            train_fe, val_fe, feats, seed=seed, use_weights=use_weights\n",
    "        )\n",
    "\n",
    "        _, nd_train, _ = eval_ranker(model, train_fe, feats_used, k=5)\n",
    "        _, nd_val, _ = eval_ranker(model, val_fe, feats_used, k=5)\n",
    "        _, nd_test, nd_test_all_sessions = eval_ranker(model, test_fe, feats_used, k=5)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"seed\": seed,\n",
    "                \"ndcg@5_train\": nd_train,\n",
    "                \"ndcg@5_val\": nd_val,\n",
    "                \"ndcg@5_test\": nd_test,\n",
    "                \"ndcg@5_test_all_sessions\": nd_test_all_sessions,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "cv = run_cv(df, seeds=(11, 22, 33, 44, 55), m=50.0, use_ipw=False, use_weights=False, val_fraction=0.1)\n",
    "display(cv)\n",
    "\n",
    "print(\"\\nMean  std:\")\n",
    "print(\"NDCG@5 test:\", float(cv[\"ndcg@5_test\"].mean()), \"\", float(cv[\"ndcg@5_test\"].std()))\n",
    "print(\n",
    "    \"NDCG@5 test (include all sessions, no-click => 0):\",\n",
    "    float(cv[\"ndcg@5_test_all_sessions\"].mean()),\n",
    "    \"\",\n",
    "    float(cv[\"ndcg@5_test_all_sessions\"].std()),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a751e6b",
   "metadata": {},
   "source": [
    "## 5) Optional: IPW experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c3050eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3229\n",
      "[LightGBM] [Info] Number of data points in the train set: 48690, number of used features: 42\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.330079\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.338012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2713\n",
      "[LightGBM] [Info] Number of data points in the train set: 48690, number of used features: 37\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.328959\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's ndcg@5: 0.342967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3234\n",
      "[LightGBM] [Info] Number of data points in the train set: 48703, number of used features: 42\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.36972\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's ndcg@5: 0.382627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2973\n",
      "[LightGBM] [Info] Number of data points in the train set: 48703, number of used features: 38\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.367406\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's ndcg@5: 0.38254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3230\n",
      "[LightGBM] [Info] Number of data points in the train set: 48658, number of used features: 42\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.320564\n",
      "[600]\tvalid_0's ndcg@5: 0.313583\n",
      "Early stopping, best iteration is:\n",
      "[332]\tvalid_0's ndcg@5: 0.321948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 48658, number of used features: 38\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.321037\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's ndcg@5: 0.328334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3234\n",
      "[LightGBM] [Info] Number of data points in the train set: 48623, number of used features: 42\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.341048\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's ndcg@5: 0.345363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2973\n",
      "[LightGBM] [Info] Number of data points in the train set: 48623, number of used features: 38\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.342109\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's ndcg@5: 0.346817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/var/folders/v6/xzrdvw71737cqqc8m_0c1s1w0000gp/T/ipykernel_69803/3076968918.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  prop = train.groupby(\"position_bucket\")[\"clicked\"].mean().to_dict()\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3233\n",
      "[LightGBM] [Info] Number of data points in the train set: 48694, number of used features: 42\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.324913\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's ndcg@5: 0.334569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2972\n",
      "[LightGBM] [Info] Number of data points in the train set: 48694, number of used features: 38\n",
      "[LightGBM] [Info] Calculating query weights...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.332074\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's ndcg@5: 0.336931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>ndcg@5_train</th>\n",
       "      <th>ndcg@5_val</th>\n",
       "      <th>ndcg@5_test</th>\n",
       "      <th>ndcg@5_test_all_sessions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0.738628</td>\n",
       "      <td>0.634194</td>\n",
       "      <td>0.656016</td>\n",
       "      <td>0.308737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.699924</td>\n",
       "      <td>0.669661</td>\n",
       "      <td>0.632061</td>\n",
       "      <td>0.293118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>0.692354</td>\n",
       "      <td>0.633811</td>\n",
       "      <td>0.655584</td>\n",
       "      <td>0.323694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>0.720651</td>\n",
       "      <td>0.655781</td>\n",
       "      <td>0.644240</td>\n",
       "      <td>0.308027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>0.806906</td>\n",
       "      <td>0.649558</td>\n",
       "      <td>0.637475</td>\n",
       "      <td>0.300410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  ndcg@5_train  ndcg@5_val  ndcg@5_test  ndcg@5_test_all_sessions\n",
       "0    11      0.738628    0.634194     0.656016                  0.308737\n",
       "1    22      0.699924    0.669661     0.632061                  0.293118\n",
       "2    33      0.692354    0.633811     0.655584                  0.323694\n",
       "3    44      0.720651    0.655781     0.644240                  0.308027\n",
       "4    55      0.806906    0.649558     0.637475                  0.300410"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean  std (IPW):\n",
      "NDCG@5 test: 0.6450750885781898  0.010699934343641573\n",
      "NDCG@5 test (include all sessions, no-click => 0): 0.3067974988344176  0.011387521640854813\n"
     ]
    }
   ],
   "source": [
    "cv_ipw = run_cv(df, seeds=(11, 22, 33, 44, 55), m=50.0, use_ipw=True, use_weights=True, val_fraction=0.1)\n",
    "display(cv_ipw)\n",
    "\n",
    "print(\"\\nMean  std (IPW):\")\n",
    "print(\"NDCG@5 test:\", float(cv_ipw[\"ndcg@5_test\"].mean()), \"\", float(cv_ipw[\"ndcg@5_test\"].std()))\n",
    "print(\n",
    "    \"NDCG@5 test (include all sessions, no-click => 0):\",\n",
    "    float(cv_ipw[\"ndcg@5_test_all_sessions\"].mean()),\n",
    "    \"\",\n",
    "    float(cv_ipw[\"ndcg@5_test_all_sessions\"].std()),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6e1ce",
   "metadata": {},
   "source": [
    "## 6) Export predictions.csv (seed=42 test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69015159",
   "metadata": {},
   "source": [
    "## Part 3: Business summary & results export\n",
    "\n",
    "Generate `results.json` with all required sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718ed209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Results:\n",
      "{\n",
      "  \"candidate_info\": {\n",
      "    \"language_used\": \"Python\",\n",
      "    \"time_spent_hours\": 8.0\n",
      "  },\n",
      "  \"data_analysis\": {\n",
      "    \"overall_ctr\": 0.0767,\n",
      "    \"position_bias_ratio\": 3.42,\n",
      "    \"electronics_ctr\": 0.1219,\n",
      "    \"quality_correlation\": 0.1198,\n",
      "    \"best_category\": \"Elektronika\"\n",
      "  },\n",
      "  \"model_performance\": {\n",
      "    \"algorithm_used\": \"LightGBM\",\n",
      "    \"ndcg_at_5\": 0.6222,\n",
      "    \"features_count\": 38,\n",
      "    \"top_features\": [\n",
      "      \"quality_minus_session_mean\",\n",
      "      \"quality_x_log_price\"\n",
      "    ]\n",
      "  },\n",
      "  \"business_analysis\": {\n",
      "    \"expected_ctr_lift_percent\": 15\n",
      "  }\n",
      "}\n",
      "\n",
      " results.json saved successfully\n",
      " predictions.csv saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Generate complete results.json\n",
    "\n",
    "TIME_SPENT_HOURS = 8.0  # keep consistent across artifacts\n",
    "\n",
    "# IMPORTANT: report TEST NDCG@5 (not training), using the standard convention of\n",
    "# excluding sessions with no positive labels (no-click sessions).\n",
    "model_performance = {\n",
    "    \"algorithm_used\": \"LightGBM\",\n",
    "    \"ndcg_at_5\": round(float(ndcg_test), 4),\n",
    "    \"features_count\": int(len(feature_cols)),\n",
    "    \"top_features\": imp.head(2).index.tolist(),\n",
    "}\n",
    "\n",
    "# Business Analysis\n",
    "# Note: mapping offline NDCG to CTR lift is not direct; treat this as a hypothesis to validate via A/B.\n",
    "expected_ctr_lift_percent = 15\n",
    "\n",
    "\n",
    "business_analysis = {\n",
    "    \"expected_ctr_lift_percent\": expected_ctr_lift_percent,\n",
    "}\n",
    "\n",
    "candidate_info = {\n",
    "    \"language_used\": \"Python\",\n",
    "    \"time_spent_hours\": TIME_SPENT_HOURS,\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"candidate_info\": candidate_info,\n",
    "    \"data_analysis\": data_analysis,\n",
    "    \"model_performance\": model_performance,\n",
    "    \"business_analysis\": business_analysis,\n",
    "}\n",
    "\n",
    "with open(\"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Complete Results:\")\n",
    "print(json.dumps(results, indent=2, ensure_ascii=False))\n",
    "print(f\"\\n results.json saved successfully\")\n",
    "print(f\" predictions.csv saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b4c0ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>actual_clicked</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>prod_17_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>prod_17_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.288326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>prod_17_3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.403197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>prod_17_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>prod_17_5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.234676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id product_id  actual_clicked  predicted_score\n",
       "0          17  prod_17_1               0         0.131145\n",
       "1          17  prod_17_2               0         0.288326\n",
       "2          17  prod_17_3               0        -0.403197\n",
       "3          17  prod_17_4               0         0.058693\n",
       "4          17  prod_17_5               0        -0.234676"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_out = test_fe[[\"session_id\",\"product_id\"]].copy()\n",
    "pred_out[\"actual_clicked\"] = test_fe[\"clicked\"].astype(int).to_numpy()\n",
    "pred_out[\"predicted_score\"] = pred\n",
    "pred_out.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "pred_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed49dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3229\n",
      "[LightGBM] [Info] Number of data points in the train set: 48690, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.810665\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.821421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2713\n",
      "[LightGBM] [Info] Number of data points in the train set: 48690, number of used features: 37\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.805974\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@5: 0.818302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3202\n",
      "[LightGBM] [Info] Number of data points in the train set: 48690, number of used features: 34\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.755282\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's ndcg@5: 0.763265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2941\n",
      "[LightGBM] [Info] Number of data points in the train set: 48690, number of used features: 30\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.743602\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@5: 0.762636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3234\n",
      "[LightGBM] [Info] Number of data points in the train set: 48703, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.817622\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's ndcg@5: 0.828105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2973\n",
      "[LightGBM] [Info] Number of data points in the train set: 48703, number of used features: 38\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.814411\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's ndcg@5: 0.828887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3207\n",
      "[LightGBM] [Info] Number of data points in the train set: 48703, number of used features: 34\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.749636\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.760348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2691\n",
      "[LightGBM] [Info] Number of data points in the train set: 48703, number of used features: 29\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.747026\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.76632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3230\n",
      "[LightGBM] [Info] Number of data points in the train set: 48658, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.824937\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's ndcg@5: 0.830073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2969\n",
      "[LightGBM] [Info] Number of data points in the train set: 48658, number of used features: 38\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.826509\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's ndcg@5: 0.832446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3203\n",
      "[LightGBM] [Info] Number of data points in the train set: 48658, number of used features: 34\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.764644\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.78043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2942\n",
      "[LightGBM] [Info] Number of data points in the train set: 48658, number of used features: 30\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.755366\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.781416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3234\n",
      "[LightGBM] [Info] Number of data points in the train set: 48623, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.823675\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@5: 0.837284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2973\n",
      "[LightGBM] [Info] Number of data points in the train set: 48623, number of used features: 38\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.828681\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's ndcg@5: 0.836097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3207\n",
      "[LightGBM] [Info] Number of data points in the train set: 48623, number of used features: 34\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.769043\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.794501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2691\n",
      "[LightGBM] [Info] Number of data points in the train set: 48623, number of used features: 29\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.770978\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@5: 0.80075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3233\n",
      "[LightGBM] [Info] Number of data points in the train set: 48694, number of used features: 42\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.828638\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@5: 0.835197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2717\n",
      "[LightGBM] [Info] Number of data points in the train set: 48694, number of used features: 37\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.832037\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's ndcg@5: 0.836751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3206\n",
      "[LightGBM] [Info] Number of data points in the train set: 48694, number of used features: 34\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.764734\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's ndcg@5: 0.783521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2945\n",
      "[LightGBM] [Info] Number of data points in the train set: 48694, number of used features: 30\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's ndcg@5: 0.773765\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's ndcg@5: 0.789198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/data-science/lib/python3.11/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>ndcg@5_test_pos_only</th>\n",
       "      <th>ndcg@5_test_qpr_only</th>\n",
       "      <th>ndcg@5_test_full</th>\n",
       "      <th>ndcg@5_test_no_pos</th>\n",
       "      <th>ndcg@5_test_full_all_sessions</th>\n",
       "      <th>ndcg@5_test_no_pos_all_sessions</th>\n",
       "      <th>n_features_full</th>\n",
       "      <th>n_features_no_pos</th>\n",
       "      <th>pos_feature_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0.628691</td>\n",
       "      <td>0.518285</td>\n",
       "      <td>0.650277</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.306036</td>\n",
       "      <td>0.251860</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.616157</td>\n",
       "      <td>0.496397</td>\n",
       "      <td>0.639958</td>\n",
       "      <td>0.522342</td>\n",
       "      <td>0.296781</td>\n",
       "      <td>0.242236</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>0.631010</td>\n",
       "      <td>0.523750</td>\n",
       "      <td>0.653875</td>\n",
       "      <td>0.567008</td>\n",
       "      <td>0.322851</td>\n",
       "      <td>0.279960</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>0.627556</td>\n",
       "      <td>0.497434</td>\n",
       "      <td>0.647425</td>\n",
       "      <td>0.538117</td>\n",
       "      <td>0.309550</td>\n",
       "      <td>0.257287</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>0.609911</td>\n",
       "      <td>0.508788</td>\n",
       "      <td>0.636519</td>\n",
       "      <td>0.516417</td>\n",
       "      <td>0.299960</td>\n",
       "      <td>0.243362</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  ndcg@5_test_pos_only  ndcg@5_test_qpr_only  ndcg@5_test_full  ndcg@5_test_no_pos  ndcg@5_test_full_all_sessions  ndcg@5_test_no_pos_all_sessions  n_features_full  n_features_no_pos  \\\n",
       "0    11              0.628691              0.518285          0.650277            0.535162                       0.306036                         0.251860               37                 30   \n",
       "1    22              0.616157              0.496397          0.639958            0.522342                       0.296781                         0.242236               38                 29   \n",
       "2    33              0.631010              0.523750          0.653875            0.567008                       0.322851                         0.279960               38                 30   \n",
       "3    44              0.627556              0.497434          0.647425            0.538117                       0.309550                         0.257287               38                 29   \n",
       "4    55              0.609911              0.508788          0.636519            0.516417                       0.299960                         0.243362               37                 30   \n",
       "\n",
       "   pos_feature_count  \n",
       "0                  8  \n",
       "1                  8  \n",
       "2                  8  \n",
       "3                  8  \n",
       "4                  8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean  std (CV, TEST):\n",
      "ndcg@5_test_pos_only: 0.6227  0.0091\n",
      "ndcg@5_test_qpr_only: 0.5089  0.0122\n",
      "ndcg@5_test_full: 0.6456  0.0072\n",
      "ndcg@5_test_no_pos: 0.5358  0.0196\n",
      "ndcg@5_test_full_all_sessions: 0.3070  0.0102\n",
      "ndcg@5_test_no_pos_all_sessions: 0.2549  0.0153\n"
     ]
    }
   ],
   "source": [
    "# --- Robustness checks: position baseline + no-position ablation ---\n",
    "\n",
    "\n",
    "def _split_feature_sets(feature_cols):\n",
    "    \"\"\"Return (position_related, non_position) feature lists.\"\"\"\n",
    "    pos_feats = []\n",
    "    for c in feature_cols:\n",
    "        if (\n",
    "            c.startswith(\"pos_\")\n",
    "            or c.startswith(\"posb_\")\n",
    "            or (\"position_bucket\" in c)\n",
    "            or (\"_posb_\" in c)\n",
    "            or (c in {\"pos_boost_clipped3\", \"ctr_prior_cat_x_posb\"})\n",
    "        ):\n",
    "            pos_feats.append(c)\n",
    "\n",
    "    pos_feats = sorted(set(pos_feats))\n",
    "    non_pos = [c for c in feature_cols if c not in set(pos_feats)]\n",
    "    return pos_feats, non_pos\n",
    "\n",
    "\n",
    "def run_cv_feature_sets(\n",
    "    df,\n",
    "    seeds=(11, 22, 33, 44, 55),\n",
    "    m=50.0,\n",
    "    k=5,\n",
    "    val_fraction=0.1,\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        train_full_raw, test_raw = split_by_session(df, seed=seed)\n",
    "        train_raw, val_raw = split_train_val_from_train(train_full_raw, seed=seed, val_fraction=val_fraction)\n",
    "\n",
    "        train_fe, val_fe, feats_full, _ = build_features_v4(train_raw, val_raw, m=m, use_ipw=False)\n",
    "        _, test_fe, _, _ = build_features_v4(train_raw, test_raw, m=m, use_ipw=False)\n",
    "\n",
    "        pos_feats, feats_no_pos = _split_feature_sets(feats_full)\n",
    "\n",
    "        # Baselines on TEST\n",
    "        y = test_fe[\"clicked\"].astype(int).to_numpy()\n",
    "        sess = test_fe[\"session_id\"].to_numpy()\n",
    "\n",
    "        score_pos_only = test_fe[\"pos_boost_clipped3\"].to_numpy()\n",
    "        ndcg_pos_only = ndcg_at_k(y, score_pos_only, sess, k=k, ignore_no_positive=True)\n",
    "\n",
    "        score_qpr = test_fe[\"quality_price_ratio\"].to_numpy()\n",
    "        ndcg_qpr = ndcg_at_k(y, score_qpr, sess, k=k, ignore_no_positive=True)\n",
    "\n",
    "        # Full model (train+val protocol)\n",
    "        model_full, _, feats_full_used, _ = train_ranker_with_pruning(\n",
    "            train_fe, val_fe, feats_full, seed=seed, use_weights=False\n",
    "        )\n",
    "        _, ndcg_full, ndcg_full_all_sessions = eval_ranker(model_full, test_fe, feats_full_used, k=k)\n",
    "\n",
    "        # No-position-features model\n",
    "        model_nopos, _, feats_no_pos_used, _ = train_ranker_with_pruning(\n",
    "            train_fe, val_fe, feats_no_pos, seed=seed, use_weights=False\n",
    "        )\n",
    "        _, ndcg_nopos, ndcg_nopos_all_sessions = eval_ranker(model_nopos, test_fe, feats_no_pos_used, k=k)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"seed\": seed,\n",
    "                \"ndcg@5_test_pos_only\": float(ndcg_pos_only),\n",
    "                \"ndcg@5_test_qpr_only\": float(ndcg_qpr),\n",
    "                \"ndcg@5_test_full\": float(ndcg_full),\n",
    "                \"ndcg@5_test_no_pos\": float(ndcg_nopos),\n",
    "                \"ndcg@5_test_full_all_sessions\": float(ndcg_full_all_sessions),\n",
    "                \"ndcg@5_test_no_pos_all_sessions\": float(ndcg_nopos_all_sessions),\n",
    "                \"n_features_full\": int(len(feats_full_used)),\n",
    "                \"n_features_no_pos\": int(len(feats_no_pos_used)),\n",
    "                \"pos_feature_count\": int(len(pos_feats)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    display(out)\n",
    "\n",
    "    def _mean_std(col):\n",
    "        return float(out[col].mean()), float(out[col].std())\n",
    "\n",
    "    print(\"\\nMean  std (CV, TEST):\")\n",
    "    for col in [\n",
    "        \"ndcg@5_test_pos_only\",\n",
    "        \"ndcg@5_test_qpr_only\",\n",
    "        \"ndcg@5_test_full\",\n",
    "        \"ndcg@5_test_no_pos\",\n",
    "        \"ndcg@5_test_full_all_sessions\",\n",
    "        \"ndcg@5_test_no_pos_all_sessions\",\n",
    "    ]:\n",
    "        mu, sd = _mean_std(col)\n",
    "        print(f\"{col}: {mu:.4f}  {sd:.4f}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "robust_cv = run_cv_feature_sets(df, seeds=(11, 22, 33, 44, 55), m=50.0, k=5, val_fraction=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
